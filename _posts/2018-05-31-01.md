---
layout: post
title: 使用Keras进行自然语言处理-数据预处理
description: 建立token字典，转换为数字列表，padding
category: blog
---

这篇笔记简单记录一点关于使用Keras进行自然语言处理的内容。由于一个实验的需要，需要对一些字符串进行简单的自然语言处理。具体而言，我们需要把待分析字符串的每一个字符看作一个词，如此，这个字符串就转换成了有若干个词的“一段话”或“一段文章”。这样就可以对这个字符串采用其他自然语言处理方法来进行分析。

实验所用的深度学习框架是Keras，后端为TensorFlow。在Keras中，上述的处理流程十分简洁，步骤为：

1. 读取数据

2. 建立token字典

3. 使用token把文字转换为数字列表

4. 截长补短padding

5. 后续处理

## 第一步：读取数据

使用`read_files()`函数读取数据，最基础的功能代码如下：

``` python
def read_files():
    fi = open('data.txt', 'rU')
    all_texts = [line.strip() for line in fi.readlines()]
    return all_texts
```
建立all_texts列表，列表的每一项是一串字符串，视其为一段文章。

## 第二步：建立token字典

首先导入keras模块：

``` python
from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
```

建立token：

``` python
token = Tokenizer(num_words=256, filters='\n', lower=True, char_level=True)
token.fit_on_texts(train_text)
```

* `num_words`为字典的单词个数，因为在此问题场景中，我们把每个字符看作一个独立的单词，ascii只有256种字符，因此字典大小为256。

* `filters`是过滤器。

* `lower=True`转换为小写。

* `char_level`即为是否把单个字符视为单词，也就是逐字符处理。

还可以通过print出`token.document_count`查看读取了多少段文章，`token.word_index`查看字典的建立情况。

## 第三步：转换为数字列表

使用keras token的内置方法：

``` python
x_train_seq = token.texts_to_sequences(train_text)
```

把文字转换为数字列表。`print(x_train_seq)`即可查看转换情况。

## 第四步：padding

Keras的padding是截长补短的。padding代码为：

```python
x_train = sequence.pad_sequences(x_train_seq, maxlen = 100)
```

`maxlen`规定padding的长度。

这就是keras的数据预处理的主要步骤和基本代码。

> 关于keras的文字预处理详细内容，可参考keras文档：[http://keras-cn.readthedocs.io/en/latest/preprocessing/text/](http://keras-cn.readthedocs.io/en/latest/preprocessing/text/)